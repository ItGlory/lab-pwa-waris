"""
LLM Service for AI Chat functionality
Supports Ollama for local development and can be extended for production LLMs
"""

import os
import httpx
from typing import AsyncGenerator, Optional
import json


# Configuration
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434")
LLM_MODEL = os.getenv("LLM_MODEL", "llama3.2")  # Default model

# System prompt for WARIS context
WARIS_SYSTEM_PROMPT = """‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ WARIS AI Assistant ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢
‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏õ‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏†‡∏π‡∏°‡∏¥‡∏†‡∏≤‡∏Ñ (‡∏Å‡∏õ‡∏†.) ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢

‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:
1. ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏à‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥‡∏¢‡πà‡∏≠‡∏¢ (DMA)
2. ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢
3. ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
4. ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏õ‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ô‡πâ‡∏≥

‡∏Å‡∏é‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏™‡∏°‡∏≠ ‡∏¢‡∏Å‡πÄ‡∏ß‡πâ‡∏ô‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ
- ‡∏ï‡∏≠‡∏ö‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô
- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤
- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ï‡∏£‡∏á‡πÜ
"""


class LLMService:
    """Service for interacting with LLM backends"""

    def __init__(self):
        self.ollama_url = OLLAMA_URL
        self.model = LLM_MODEL
        self.timeout = httpx.Timeout(60.0, connect=10.0)

    async def check_ollama_available(self) -> bool:
        """Check if Ollama server is available"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                return response.status_code == 200
        except Exception:
            return False

    async def get_available_models(self) -> list[str]:
        """Get list of available models from Ollama"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    return [m["name"] for m in data.get("models", [])]
        except Exception:
            pass
        return []

    async def stream_chat(
        self,
        message: str,
        conversation_history: Optional[list[dict]] = None,
        context: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Stream a chat response from the LLM

        Args:
            message: The user's message
            conversation_history: Previous messages in the conversation
            context: Additional context (e.g., DMA data, alerts)
        """
        # Build messages array
        messages = [{"role": "system", "content": WARIS_SYSTEM_PROMPT}]

        # Add context if provided
        if context:
            messages.append({
                "role": "system",
                "content": f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:\n{context}"
            })

        # Add conversation history
        if conversation_history:
            messages.extend(conversation_history)

        # Add current message
        messages.append({"role": "user", "content": message})

        # Try Ollama first
        if await self.check_ollama_available():
            async for chunk in self._stream_ollama(messages):
                yield chunk
        else:
            # Fallback to mock response when Ollama is not available
            async for chunk in self._mock_response(message):
                yield chunk

    async def _stream_ollama(self, messages: list[dict]) -> AsyncGenerator[str, None]:
        """Stream response from Ollama"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST",
                    f"{self.ollama_url}/api/chat",
                    json={
                        "model": self.model,
                        "messages": messages,
                        "stream": True,
                    },
                ) as response:
                    async for line in response.aiter_lines():
                        if line:
                            try:
                                data = json.loads(line)
                                if "message" in data and "content" in data["message"]:
                                    yield data["message"]["content"]
                            except json.JSONDecodeError:
                                continue
        except Exception as e:
            yield f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ LLM: {str(e)}"

    async def _mock_response(self, message: str) -> AsyncGenerator[str, None]:
        """Provide mock responses when LLM is not available"""
        import asyncio

        # Thai mock responses based on keywords
        responses = {
            "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå": [
                "üìä **‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢**\n\n",
                "‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡∏û‡∏ö‡∏ß‡πà‡∏≤:\n\n",
                "- ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: **15.5%**\n",
                "- ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏Å‡∏ï‡∏¥: 54 DMA\n",
                "- ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á: 8 DMA\n",
                "- ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡∏Å‡∏§‡∏ï: 3 DMA\n\n",
                "**‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**\n",
                "1. DMA ‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ-01 (28.5%)\n",
                "2. DMA ‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà-03 (22.1%)\n",
            ],
            "‡∏™‡∏£‡∏∏‡∏õ": [
                "üîî **‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ**\n\n",
                "- ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏¥‡∏Å‡∏§‡∏ï: 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
                "- ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á: 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
                "- ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á: 2 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n\n",
                "‡∏Ñ‡∏ß‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö DMA ‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ-01 ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å\n",
            ],
            "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥": [
                "üí° **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢**\n\n",
                "1. **‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏£‡∏±‡πà‡∏ß‡∏ã‡∏∂‡∏°** - ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏£‡∏±‡πà‡∏ß\n",
                "2. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏£‡∏á‡∏î‡∏±‡∏ô‡∏ô‡πâ‡∏≥** - ‡∏•‡∏î‡πÅ‡∏£‡∏á‡∏î‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô 20-30%\n",
                "3. **‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ó‡πà‡∏≠‡πÄ‡∏Å‡πà‡∏≤** - ‡∏ó‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏≤‡∏¢‡∏∏‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 30 ‡∏õ‡∏µ\n",
                "4. **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á DMA meter** - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏∏‡∏î‡∏ß‡∏±‡∏î‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á\n\n",
                "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÑ‡∏î‡πâ 8-12%\n",
            ],
        }

        # Find matching response
        selected = None
        for keyword, response in responses.items():
            if keyword in message:
                selected = response
                break

        if not selected:
            selected = [
                "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ WARIS AI Assistant\n\n",
                "‡∏ú‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö:\n",
                "- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢\n",
                "- ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô\n",
                "- ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢\n\n",
                "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö?\n",
                "\n_‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î Offline_",
            ]

        # Stream the response
        for chunk in selected:
            await asyncio.sleep(0.03 + 0.05 * (len(chunk) / 20))
            yield chunk


# Singleton instance
llm_service = LLMService()
