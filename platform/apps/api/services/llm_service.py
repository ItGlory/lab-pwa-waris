"""
LLM Service for AI Chat functionality
Supports OpenRouter (default) and Ollama (optional fallback)
"""

import httpx
from typing import AsyncGenerator, Optional
import json
from enum import Enum

from core.llm_config import llm_settings, WARIS_SYSTEM_PROMPT, DEFAULT_MODEL_PRIORITY


class LLMProvider(str, Enum):
    """Available LLM providers"""
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    AUTO = "auto"  # Try OpenRouter first, fallback to Ollama


class LLMService:
    """Service for interacting with LLM backends (OpenRouter and Ollama)"""

    def __init__(self):
        # Provider configuration
        self.provider = LLMProvider(llm_settings.LLM_PROVIDER.lower())

        # OpenRouter configuration
        self.openrouter_base_url = llm_settings.OPENROUTER_BASE_URL
        self.openrouter_api_key = llm_settings.OPENROUTER_API_KEY
        self.openrouter_model = llm_settings.OPENROUTER_DEFAULT_MODEL

        # Ollama configuration (fallback)
        self.ollama_url = llm_settings.OLLAMA_BASE_URL
        self.ollama_model = llm_settings.OLLAMA_DEFAULT_MODEL

        # General settings
        self.max_tokens = llm_settings.LLM_MAX_TOKENS
        self.temperature = llm_settings.LLM_TEMPERATURE
        self.timeout = httpx.Timeout(float(llm_settings.LLM_TIMEOUT), connect=10.0)

        # Current model info
        self._current_provider: Optional[LLMProvider] = None
        self._current_model: Optional[str] = None

    @property
    def model(self) -> str:
        """Get current active model name"""
        if self._current_model:
            return self._current_model
        if self.provider == LLMProvider.OLLAMA:
            return self.ollama_model
        return self.openrouter_model

    @property
    def active_provider(self) -> str:
        """Get current active provider"""
        if self._current_provider:
            return self._current_provider.value
        return self.provider.value

    async def check_openrouter_available(self) -> bool:
        """Check if OpenRouter API is available"""
        if not self.openrouter_api_key:
            return False
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(
                    f"{self.openrouter_base_url}/models",
                    headers={"Authorization": f"Bearer {self.openrouter_api_key}"}
                )
                return response.status_code == 200
        except Exception:
            return False

    async def check_ollama_available(self) -> bool:
        """Check if Ollama server is available"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                return response.status_code == 200
        except Exception:
            return False

    async def get_available_models(self) -> list[str]:
        """Get list of available models based on provider"""
        models = []

        # Check OpenRouter models
        if self.provider in [LLMProvider.OPENROUTER, LLMProvider.AUTO]:
            if await self.check_openrouter_available():
                models.extend(DEFAULT_MODEL_PRIORITY)

        # Check Ollama models
        if self.provider in [LLMProvider.OLLAMA, LLMProvider.AUTO]:
            try:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.get(f"{self.ollama_url}/api/tags")
                    if response.status_code == 200:
                        data = response.json()
                        ollama_models = [f"ollama/{m['name']}" for m in data.get("models", [])]
                        models.extend(ollama_models)
            except Exception:
                pass

        return models

    async def get_status(self) -> dict:
        """Get LLM service status"""
        openrouter_available = await self.check_openrouter_available()
        ollama_available = await self.check_ollama_available()

        return {
            "provider": self.provider.value,
            "openrouter": {
                "available": openrouter_available,
                "model": self.openrouter_model,
                "has_api_key": bool(self.openrouter_api_key),
            },
            "ollama": {
                "available": ollama_available,
                "model": self.ollama_model,
                "url": self.ollama_url,
            },
            "active_model": self.model,
        }

    async def stream_chat(
        self,
        message: str,
        conversation_history: Optional[list[dict]] = None,
        context: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Stream a chat response from the LLM

        Args:
            message: The user's message
            conversation_history: Previous messages in the conversation
            context: Additional context (e.g., DMA data, alerts)
        """
        # Build messages array
        messages = [{"role": "system", "content": WARIS_SYSTEM_PROMPT}]

        # Add context if provided
        if context:
            messages.append({
                "role": "system",
                "content": f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:\n{context}"
            })

        # Add conversation history
        if conversation_history:
            messages.extend(conversation_history)

        # Add current message
        messages.append({"role": "user", "content": message})

        # Select provider based on configuration
        if self.provider == LLMProvider.OPENROUTER:
            # Use OpenRouter only
            if await self.check_openrouter_available():
                async for chunk in self._stream_openrouter(messages):
                    yield chunk
            else:
                yield "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ OpenRouter API ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API key ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠"

        elif self.provider == LLMProvider.OLLAMA:
            # Use Ollama only
            if await self.check_ollama_available():
                async for chunk in self._stream_ollama(messages):
                    yield chunk
            else:
                yield "‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ Ollama server ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà"

        else:  # AUTO mode - try OpenRouter first, fallback to Ollama
            if await self.check_openrouter_available():
                self._current_provider = LLMProvider.OPENROUTER
                self._current_model = self.openrouter_model
                async for chunk in self._stream_openrouter(messages):
                    yield chunk
            elif await self.check_ollama_available():
                self._current_provider = LLMProvider.OLLAMA
                self._current_model = self.ollama_model
                async for chunk in self._stream_ollama(messages):
                    yield chunk
            else:
                # Fallback to mock response when no LLM is available
                async for chunk in self._mock_response(message):
                    yield chunk

    async def _stream_openrouter(self, messages: list[dict]) -> AsyncGenerator[str, None]:
        """Stream response from OpenRouter API"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST",
                    f"{self.openrouter_base_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.openrouter_api_key}",
                        "Content-Type": "application/json",
                        "HTTP-Referer": "https://waris.pwa.co.th",
                        "X-Title": "WARIS - Water Loss Analysis System",
                    },
                    json={
                        "model": self.openrouter_model,
                        "messages": messages,
                        "stream": True,
                        "max_tokens": self.max_tokens,
                        "temperature": self.temperature,
                    },
                ) as response:
                    if response.status_code != 200:
                        error_body = await response.aread()
                        yield f"OpenRouter API error: {response.status_code} - {error_body.decode()}"
                        return

                    async for line in response.aiter_lines():
                        if line.startswith("data: "):
                            data_str = line[6:]
                            if data_str.strip() == "[DONE]":
                                break
                            try:
                                data = json.loads(data_str)
                                if "choices" in data and len(data["choices"]) > 0:
                                    delta = data["choices"][0].get("delta", {})
                                    if "content" in delta:
                                        yield delta["content"]
                            except json.JSONDecodeError:
                                continue
        except Exception as e:
            yield f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ OpenRouter: {str(e)}"

    async def _stream_ollama(self, messages: list[dict]) -> AsyncGenerator[str, None]:
        """Stream response from Ollama"""
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                async with client.stream(
                    "POST",
                    f"{self.ollama_url}/api/chat",
                    json={
                        "model": self.ollama_model,
                        "messages": messages,
                        "stream": True,
                        "options": {
                            "num_predict": self.max_tokens,
                            "temperature": self.temperature,
                        },
                    },
                ) as response:
                    async for line in response.aiter_lines():
                        if line:
                            try:
                                data = json.loads(line)
                                if "message" in data and "content" in data["message"]:
                                    yield data["message"]["content"]
                            except json.JSONDecodeError:
                                continue
        except Exception as e:
            yield f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Ollama: {str(e)}"

    async def _mock_response(self, message: str) -> AsyncGenerator[str, None]:
        """Provide mock responses when no LLM is available"""
        import asyncio

        # Thai mock responses based on keywords
        responses = {
            "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå": [
                "üìä **‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢**\n\n",
                "‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î ‡∏û‡∏ö‡∏ß‡πà‡∏≤:\n\n",
                "- ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: **15.5%**\n",
                "- ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏Å‡∏ï‡∏¥: 54 DMA\n",
                "- ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á: 8 DMA\n",
                "- ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡∏Å‡∏§‡∏ï: 3 DMA\n\n",
                "**‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**\n",
                "1. DMA ‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ-01 (28.5%)\n",
                "2. DMA ‡πÄ‡∏ä‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà-03 (22.1%)\n",
            ],
            "‡∏™‡∏£‡∏∏‡∏õ": [
                "üîî **‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ**\n\n",
                "- ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ß‡∏¥‡∏Å‡∏§‡∏ï: 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
                "- ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏™‡∏π‡∏á: 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n",
                "- ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á: 2 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£\n\n",
                "‡∏Ñ‡∏ß‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö DMA ‡∏ä‡∏•‡∏ö‡∏∏‡∏£‡∏µ-01 ‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å\n",
            ],
            "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥": [
                "üí° **‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢**\n\n",
                "1. **‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏£‡∏±‡πà‡∏ß‡∏ã‡∏∂‡∏°** - ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏£‡∏±‡πà‡∏ß\n",
                "2. **‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏£‡∏á‡∏î‡∏±‡∏ô‡∏ô‡πâ‡∏≥** - ‡∏•‡∏î‡πÅ‡∏£‡∏á‡∏î‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏Å‡∏•‡∏≤‡∏á‡∏Ñ‡∏∑‡∏ô 20-30%\n",
                "3. **‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ó‡πà‡∏≠‡πÄ‡∏Å‡πà‡∏≤** - ‡∏ó‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏≤‡∏¢‡∏∏‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 30 ‡∏õ‡∏µ\n",
                "4. **‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á DMA meter** - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏∏‡∏î‡∏ß‡∏±‡∏î‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á\n\n",
                "‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡πÑ‡∏î‡πâ 8-12%\n",
            ],
        }

        # Find matching response
        selected = None
        for keyword, response in responses.items():
            if keyword in message:
                selected = response
                break

        if not selected:
            selected = [
                "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ú‡∏°‡∏Ñ‡∏∑‡∏≠ WARIS AI Assistant\n\n",
                "‡∏ú‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö:\n",
                "- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢\n",
                "- ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô\n",
                "- ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢\n\n",
                "‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö?\n",
                "\n_‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ‡πÇ‡∏´‡∏°‡∏î Offline - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ OpenRouter API key ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î Ollama_",
            ]

        # Stream the response
        for chunk in selected:
            await asyncio.sleep(0.03 + 0.05 * (len(chunk) / 20))
            yield chunk


# Singleton instance
llm_service = LLMService()
